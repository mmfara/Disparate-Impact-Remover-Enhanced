{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG2PKtIxD0zfqwyWcnZ02i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmfara/Disparate-Impact-Remover-Enhanced/blob/main/Disparate_Impact_Remover__Modified_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpdVRT9N24a7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import logging\n",
        "from itertools import product\n",
        "from aif360.algorithms import Transformer\n",
        "\n",
        "\n",
        "class DisparateImpactRemover(Transformer):\n",
        "    \"\"\"\n",
        "    Extended Disparate Impact Remover:\n",
        "    - Uses GeneralRepairer for rank computation (faithful to Feldman et al.)\n",
        "    - Supports intersectional groups\n",
        "    - Repairs feature values toward a shared global distribution\n",
        "    - Preserves within-group rank\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, repair_level=1.0, sensitive_attribute=None, min_group_size=20, verbose=True):\n",
        "        super().__init__()\n",
        "        from BlackBoxAuditing.repairers.GeneralRepairer import Repairer\n",
        "        self.Repairer = Repairer\n",
        "\n",
        "        self.repair_level = repair_level\n",
        "        self.min_group_size = min_group_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if not 0.0 <= repair_level <= 1.0:\n",
        "            raise ValueError(\"'repair_level' must be between 0.0 and 1.0.\")\n",
        "\n",
        "        if isinstance(sensitive_attribute, str):\n",
        "            self.sensitive_attributes = [sensitive_attribute]\n",
        "        elif isinstance(sensitive_attribute, list):\n",
        "            self.sensitive_attributes = sensitive_attribute\n",
        "        elif sensitive_attribute is None:\n",
        "            self.sensitive_attributes = []\n",
        "        else:\n",
        "            raise TypeError(\"sensitive_attribute must be str, list, or None\")\n",
        "\n",
        "        if self.verbose:\n",
        "            logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    def fit_transform(self, dataset):\n",
        "        if not self.sensitive_attributes:\n",
        "            self.sensitive_attributes = dataset.protected_attribute_names[:1]\n",
        "\n",
        "        indices = [dataset.feature_names.index(attr) for attr in self.sensitive_attributes]\n",
        "        protected_values = [np.unique(dataset.features[:, idx]) for idx in indices]\n",
        "        group_combinations = list(product(*protected_values))\n",
        "\n",
        "        repaired = dataset.copy()\n",
        "        all_features = repaired.features.copy()\n",
        "\n",
        "        feature_dim = all_features.shape[1]\n",
        "        protected_set = set(indices)\n",
        "        non_protected_indices = [i for i in range(feature_dim) if i not in protected_set]\n",
        "\n",
        "        # Step 1: Collect data for global target distribution\n",
        "        pooled_values = []\n",
        "        group_indices = {}\n",
        "\n",
        "        for group_vals in group_combinations:\n",
        "            mask = np.logical_and.reduce([\n",
        "                all_features[:, idx] == val for idx, val in zip(indices, group_vals)\n",
        "            ])\n",
        "            group_features = all_features[mask][:, non_protected_indices]\n",
        "\n",
        "            if group_features.shape[0] < self.min_group_size:\n",
        "                if self.verbose:\n",
        "                    logging.warning(f\"Skipping group {group_vals} (size={group_features.shape[0]})\")\n",
        "                continue\n",
        "\n",
        "            pooled_values.append(group_features)\n",
        "            group_indices[group_vals] = np.where(mask)[0]\n",
        "\n",
        "        if not pooled_values:\n",
        "            raise ValueError(\"No eligible groups met the minimum size requirement.\")\n",
        "\n",
        "        # Build global sorted target distribution\n",
        "        pooled_array = np.vstack(pooled_values)\n",
        "        global_sorted = np.sort(pooled_array, axis=0)\n",
        "\n",
        "        # Step 2: Repair each group by aligning its ranks to global distribution\n",
        "        for group_vals, idxs in group_indices.items():\n",
        "            group_data = all_features[idxs][:, non_protected_indices]\n",
        "            n = len(idxs)\n",
        "\n",
        "            # Use GeneralRepairer to compute feature-wise ranks\n",
        "            repairer = self.Repairer(group_data.tolist(), 0, 1.0, False)\n",
        "            ranked = np.array(repairer.repair(group_data.tolist()))\n",
        "\n",
        "            # Use the ranks to align to global distribution\n",
        "            ranks = np.argsort(np.argsort(group_data, axis=0))\n",
        "            aligned_by_rank = np.zeros_like(group_data)\n",
        "\n",
        "            for col in range(group_data.shape[1]):\n",
        "                aligned_by_rank[:, col] = global_sorted[ranks[:, col], col]\n",
        "\n",
        "            # Interpolate: original + global target\n",
        "            group_repaired = (\n",
        "                (1 - self.repair_level) * group_data +\n",
        "                self.repair_level * aligned_by_rank\n",
        "            )\n",
        "\n",
        "            all_features[idxs[:, None], non_protected_indices] = group_repaired\n",
        "\n",
        "            if self.verbose:\n",
        "                logging.info(f\"Repaired group {group_vals} (size={n}) toward global target.\")\n",
        "\n",
        "        # Restore protected attributes\n",
        "        for idx in indices:\n",
        "            all_features[:, idx] = dataset.features[:, idx]\n",
        "\n",
        "        repaired.features = all_features\n",
        "        return repaired\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying DisparateImpactRemover\n",
        "dir = DisparateImpactRemover(\n",
        "    repair_level=0.4,\n",
        "    sensitive_attribute=['race', 'sex'],\n",
        "    min_group_size=502,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "YnZ4FUW9CpD0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}