{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmfara/Disparate-Impact-Remover-Enhanced/blob/main/DIR%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mBvAfb4coOo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import logging\n",
        "from typing import List, Union, Set\n",
        "from aif360.algorithms import Transformer\n",
        "\n",
        "\n",
        "class DisparateImpactRemover(Transformer):\n",
        "    \"\"\"\n",
        "    Enhanced Disparate Impact Remover with:\n",
        "    - Intersectionality support\n",
        "    - min_group_size filtering\n",
        "    - groups_to_repair selective control\n",
        "    - Global repair (single Repairer call)\n",
        "    - Robust verbose logging\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 repair_level: float = 1.0,\n",
        "                 sensitive_attribute: Union[str, List[str], None] = None,\n",
        "                 min_group_size: int = 0,\n",
        "                 groups_to_repair: Union[List[str], Set[str], None] = None,\n",
        "                 verbose: bool = True):\n",
        "        super().__init__()\n",
        "        from BlackBoxAuditing.repairers.GeneralRepairer import Repairer\n",
        "        self.Repairer = Repairer\n",
        "\n",
        "        if not 0.0 <= repair_level <= 1.0:\n",
        "            raise ValueError(\"'repair_level' must be between 0.0 and 1.0.\")\n",
        "        self.repair_level = repair_level\n",
        "        self.min_group_size = max(0, min_group_size)\n",
        "        self.groups_to_repair = set(groups_to_repair) if groups_to_repair else None\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Handle sensitive attributes\n",
        "        if sensitive_attribute is None:\n",
        "            self.sensitive_attributes = []\n",
        "        elif isinstance(sensitive_attribute, str):\n",
        "            self.sensitive_attributes = [sensitive_attribute]\n",
        "        elif isinstance(sensitive_attribute, list):\n",
        "            self.sensitive_attributes = sensitive_attribute\n",
        "        else:\n",
        "            raise TypeError(\"sensitive_attribute must be str, list, or None\")\n",
        "\n",
        "        # Robust logging setup\n",
        "        self.logger = logging.getLogger(\"DisparateImpactRemover\")\n",
        "        if self.verbose:\n",
        "            self.logger.setLevel(logging.INFO)\n",
        "            if not self.logger.hasHandlers():\n",
        "                handler = logging.StreamHandler()\n",
        "                formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "                handler.setFormatter(formatter)\n",
        "                self.logger.addHandler(handler)\n",
        "\n",
        "    def fit_transform(self, dataset):\n",
        "        features = dataset.features.copy()\n",
        "        repaired = dataset.copy()\n",
        "\n",
        "        if not self.sensitive_attributes:\n",
        "            self.sensitive_attributes = dataset.protected_attribute_names[:1]\n",
        "\n",
        "        protected_indices = [dataset.feature_names.index(attr) for attr in self.sensitive_attributes]\n",
        "\n",
        "        # Generate readable group labels like \"group=1\" or \"gender=0|race=1\"\n",
        "        combined_groups = self._get_group_labels(features, dataset)\n",
        "        unique, counts = np.unique(combined_groups, return_counts=True)\n",
        "\n",
        "        if self.verbose:\n",
        "            self._log_group_stats(unique, counts)\n",
        "\n",
        "        # Determine which groups to include\n",
        "        valid_groups = []\n",
        "        for grp, cnt in zip(unique, counts):\n",
        "            reason = []\n",
        "            if cnt < self.min_group_size:\n",
        "                reason.append(f\"size {cnt} < {self.min_group_size}\")\n",
        "            if self.groups_to_repair is not None and grp not in self.groups_to_repair:\n",
        "                reason.append(\"not in groups_to_repair\")\n",
        "\n",
        "            if reason:\n",
        "                self.logger.info(f\"Skipped group: {grp} â€” {'; '.join(reason)}\")\n",
        "            else:\n",
        "                valid_groups.append(grp)\n",
        "                self.logger.info(f\"Included group: {grp} (size={cnt})\")\n",
        "\n",
        "        # Select only rows from valid groups\n",
        "        group_mask = np.isin(combined_groups, valid_groups)\n",
        "        if not np.any(group_mask):\n",
        "            raise ValueError(\"No valid rows to repair after filtering.\")\n",
        "\n",
        "        filtered_features = features[group_mask]\n",
        "        filtered_labels = combined_groups[group_mask]\n",
        "\n",
        "        # Map valid group labels to numeric codes\n",
        "        label_to_code = {label: i for i, label in enumerate(sorted(set(filtered_labels)))}\n",
        "        group_codes = np.array([label_to_code[label] for label in filtered_labels])\n",
        "\n",
        "        # Append group code to filtered features\n",
        "        repair_input = np.hstack([filtered_features, group_codes.reshape(-1, 1)])\n",
        "        repair_index = repair_input.shape[1] - 1\n",
        "\n",
        "        # Run Repairer\n",
        "        repaired_filtered = np.array(\n",
        "            self.Repairer(repair_input.tolist(), repair_index, self.repair_level, False).repair(repair_input.tolist()),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        # FIXED: Apply repair only to valid rows (no shape mismatch!)\n",
        "        repaired_features = features.copy()\n",
        "        repaired_features[group_mask] = repaired_filtered[:, :-1]\n",
        "\n",
        "        # Restore protected attributes\n",
        "        for idx in protected_indices:\n",
        "            repaired_features[:, idx] = dataset.features[:, idx]\n",
        "\n",
        "        repaired.features = repaired_features\n",
        "        return repaired\n",
        "\n",
        "    def _get_group_labels(self, features: np.ndarray, dataset) -> np.ndarray:\n",
        "        \"\"\"Create human-readable group labels from protected attributes\"\"\"\n",
        "        indices = [dataset.feature_names.index(attr) for attr in self.sensitive_attributes]\n",
        "        protected_names = [dataset.feature_names[idx] for idx in indices]\n",
        "        return np.array([\n",
        "            '|'.join(f\"{protected_names[j]}={features[i, idx]}\"\n",
        "                     for j, idx in enumerate(indices))\n",
        "            for i in range(features.shape[0])\n",
        "        ])\n",
        "\n",
        "    def _log_group_stats(self, unique, counts):\n",
        "        \"\"\"Print stats for all groups\"\"\"\n",
        "        self.logger.info(\"\\n=== Group Analysis ===\")\n",
        "        self.logger.info(f\"Protected attributes: {self.sensitive_attributes}\")\n",
        "        self.logger.info(f\"Minimum group size: {self.min_group_size}\")\n",
        "        if self.groups_to_repair:\n",
        "            self.logger.info(f\"Specific groups to repair: {self.groups_to_repair}\")\n",
        "        self.logger.info(f\"\\n{'Status'.ljust(12)} {'Group'.ljust(40)} Size\")\n",
        "        for grp, cnt in sorted(zip(unique, counts), key=lambda x: -x[1]):\n",
        "            status = []\n",
        "            if cnt < self.min_group_size:\n",
        "                status.append(\"TOO SMALL\")\n",
        "            if self.groups_to_repair and grp not in self.groups_to_repair:\n",
        "                status.append(\"NOT SELECTED\")\n",
        "            status_str = \"|\".join(status) if status else \"PROCESS\"\n",
        "            self.logger.info(f\"{status_str.ljust(12)} {grp.ljust(40)} {cnt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from my_fairness_lib import DisparateImpactRemover\n",
        "\n",
        "# Instantiate the enhanced DIR\n",
        "dir_plus = DisparateImpactRemover(\n",
        "    repair_level=1.0,                        # strength of repair\n",
        "    sensitive_attribute=[\"race\", \"gender\"],  # intersectional attributes\n",
        "    min_group_size=30,                       # optional: skip small groups\n",
        "    groups_to_repair={\"race=1|gender=0\", \"race=1|gender=1\"}  # optional: selective repair\n",
        ")\n",
        "\n",
        "# Apply it to a BinaryLabelDataset\n",
        "repaired_dataset = dir_plus.fit_transform(dataset_orig_train)\n"
      ],
      "metadata": {
        "id": "kzzdUpQQcqW0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}